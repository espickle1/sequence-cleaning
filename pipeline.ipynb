{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß¨ Protein Sequence Analysis Pipeline\n",
    "\n",
    "A complete workflow for protein sequence embedding and analysis:\n",
    "\n",
    "1. **FASTA Cleaning** - Clean sequences and parse metadata\n",
    "2. **Embedding Generation** - Generate ESM-C embeddings\n",
    "3. **Entropy Analysis** - Identify conserved and variable regions\n",
    "4. **Logits Analysis** - Analyze amino acid propensities\n",
    "5. **Export Results** - Save all outputs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Run this first!\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîß Setting up environment...\\n\")\n",
    "\n",
    "# Check environment\n",
    "try:\n",
    "    from google.colab import files as colab_files\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    # Try to clone repository (public repos only)\n",
    "    if not os.path.exists(\"sequence-cleaning\"):\n",
    "        print(\"üì• Cloning repository...\")\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"clone\", \"https://github.com/espickle1/sequence-cleaning.git\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(\"‚ö†Ô∏è Clone failed - repository may be private\")\n",
    "            print(\"\\nüìã To fix this, either:\")\n",
    "            print(\"   1. Make your GitHub repo public, OR\")\n",
    "            print(\"   2. Download repo as ZIP, upload to Colab, and run:\")\n",
    "            print(\"      !unzip sequence-cleaning-main.zip\")\n",
    "            print(\"      import os; os.chdir('sequence-cleaning-main')\")\n",
    "            raise Exception(\"Clone failed\")\n",
    "    \n",
    "    # Change to repo directory\n",
    "    os.chdir(\"sequence-cleaning\")\n",
    "    print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Install dependencies\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    !pip install -q esm huggingface_hub ipywidgets pandas torch scikit-learn matplotlib\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úÖ Running in local environment\")\n",
    "\n",
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Verify packages are available\n",
    "try:\n",
    "    from embedding import fasta_cleaner\n",
    "    from analysis import entropy_lib\n",
    "    print(\"‚úÖ Pipeline packages loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Package import failed: {e}\")\n",
    "    print(\"   Make sure you're in the sequence-cleaning directory\")\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è No GPU - running on CPU\")\n",
    "\n",
    "print(\"\\nüéâ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: FASTA Cleaning\n",
    "\n",
    "Clean protein sequences and parse metadata from FASTA headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: FASTA CLEANING\n",
    "# ============================================================\n",
    "\n",
    "from embedding.fasta_cleaner import process_fasta_files, save_results, process_fasta_content\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Storage\n",
    "sequences_df = None\n",
    "metadata_df = None\n",
    "\n",
    "# Upload widget\n",
    "fasta_upload = widgets.FileUpload(\n",
    "    accept=\".fasta,.fa,.faa,.txt\",\n",
    "    multiple=True,\n",
    "    description=\"Upload FASTA\",\n",
    "    button_style=\"primary\"\n",
    ")\n",
    "\n",
    "fasta_output = widgets.Output()\n",
    "\n",
    "def on_fasta_upload(change):\n",
    "    global sequences_df, metadata_df\n",
    "    with fasta_output:\n",
    "        clear_output()\n",
    "        if not change[\"new\"]:\n",
    "            return\n",
    "        \n",
    "        print(\"üîÑ Processing FASTA files...\")\n",
    "        \n",
    "        all_seqs = []\n",
    "        all_meta = []\n",
    "        \n",
    "        # Get uploaded data - handle different ipywidgets versions\n",
    "        uploaded = change[\"new\"]\n",
    "        \n",
    "        # ipywidgets 8.x with dict format: {filename: FileInfo}\n",
    "        if isinstance(uploaded, dict):\n",
    "            for filename, file_info in uploaded.items():\n",
    "                content = file_info[\"content\"].decode(\"utf-8\")\n",
    "                seq_df, meta_df = process_fasta_content(content, filename)\n",
    "                all_seqs.append(seq_df)\n",
    "                all_meta.append(meta_df)\n",
    "        # ipywidgets 8.x with tuple format: (FileInfo, ...)\n",
    "        elif isinstance(uploaded, tuple):\n",
    "            for file_info in uploaded:\n",
    "                filename = file_info.name\n",
    "                content = file_info.content.decode(\"utf-8\")\n",
    "                seq_df, meta_df = process_fasta_content(content, filename)\n",
    "                all_seqs.append(seq_df)\n",
    "                all_meta.append(meta_df)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected upload format: {type(uploaded)}\")\n",
    "            print(f\"   Content: {uploaded}\")\n",
    "            return\n",
    "        \n",
    "        sequences_df = pd.concat(all_seqs, ignore_index=True)\n",
    "        metadata_df = pd.concat(all_meta, ignore_index=True)\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(sequences_df)} sequences\")\n",
    "        print(f\"\\nüìã Preview:\")\n",
    "        display(sequences_df.head())\n",
    "\n",
    "fasta_upload.observe(on_fasta_upload, names=\"value\")\n",
    "\n",
    "# Display\n",
    "display(HTML(\"<h3>üìÅ Upload FASTA Files</h3>\"))\n",
    "display(fasta_upload)\n",
    "display(fasta_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned sequences\n",
    "if sequences_df is not None:\n",
    "    sequences_df.to_csv(\"sequences.csv\", index=False)\n",
    "    metadata_df.to_csv(\"metadata.csv\", index=False)\n",
    "    print(\"‚úÖ Saved sequences.csv and metadata.csv\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Upload FASTA files first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Embedding Generation\n",
    "\n",
    "Generate ESM-C protein embeddings using HuggingFace models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 2: EMBEDDING GENERATION\n# ============================================================\n\nfrom embedding.esmc_embed_lib import load_esmc_model, embed_single, save_embeddings\nfrom huggingface_hub import login\nfrom datetime import datetime\n\n# Model storage\nmodel = None\nembedding_results = None\n\n# Widgets\ntoken_input = widgets.Password(\n    placeholder=\"HuggingFace token\",\n    description=\"HF Token:\",\n    layout=widgets.Layout(width=\"400px\")\n)\n\nmodel_dropdown = widgets.Dropdown(\n    options=[(\"ESMC 600M\", \"esmc_600m\"), (\"ESMC 300M\", \"esmc_300m\")],\n    value=\"esmc_600m\",\n    description=\"Model:\"\n)\n\nload_btn = widgets.Button(description=\"üîê Load Model\", button_style=\"primary\")\nembed_btn = widgets.Button(description=\"üöÄ Generate Embeddings\", button_style=\"success\")\n\nprogress = widgets.IntProgress(value=0, min=0, max=100, description=\"Progress:\")\nembed_output = widgets.Output()\n\ndef run_embedding_for_sequence(model, seq_id, sequence):\n    \"\"\"Embed a single protein sequence and return its results.\"\"\"\n    result = embed_single(\n        model, sequence,\n        return_embeddings=True,\n        return_logits=True\n    )\n    return {\"seq_id\": seq_id, \"embeddings\": result[\"embeddings\"], \"logits\": result[\"logits\"]}\n\ndef on_load_click(btn):\n    global model\n    with embed_output:\n        clear_output()\n        print(\"üîÑ Loading model...\")\n        try:\n            model = load_esmc_model(token_input.value, model_dropdown.value)\n            print(f\"‚úÖ Model loaded on {DEVICE}\")\n        except Exception as e:\n            print(f\"‚ùå Error: {e}\")\n\ndef on_embed_click(btn):\n    global embedding_results\n    with embed_output:\n        clear_output()\n        if model is None:\n            print(\"‚ö†Ô∏è Load model first\")\n            return\n        if sequences_df is None:\n            print(\"‚ö†Ô∏è Upload FASTA first\")\n            return\n\n        print(\"üîÑ Generating embeddings per sequence...\")\n        progress.max = len(sequences_df)\n        progress.value = 0\n\n        # Build combined results dict for downstream steps\n        embedding_results = {\n            \"sequence_id\": [],\n            \"embeddings\": [],\n            \"logits\": [],\n            \"model_name\": model_dropdown.value,\n            \"created_at\": datetime.now().isoformat(),\n            \"errors\": [],\n        }\n\n        for i, (_, row) in enumerate(sequences_df.iterrows()):\n            seq_id = row[\"sequence_id\"]\n            sequence = row[\"sequence\"]\n            try:\n                result = run_embedding_for_sequence(model, seq_id, sequence)\n                embedding_results[\"sequence_id\"].append(seq_id)\n                embedding_results[\"embeddings\"].append(result[\"embeddings\"])\n                embedding_results[\"logits\"].append(result[\"logits\"])\n                print(f\"   ‚úÖ {seq_id}\")\n            except Exception as e:\n                embedding_results[\"sequence_id\"].append(seq_id)\n                embedding_results[\"embeddings\"].append(None)\n                embedding_results[\"logits\"].append(None)\n                embedding_results[\"errors\"].append((seq_id, str(e)))\n                print(f\"   ‚ùå {seq_id}: {e}\")\n\n            progress.value = i + 1\n\n        print(f\"\\n‚úÖ Embedded {len(embedding_results['sequence_id'])} sequences\")\n\nload_btn.on_click(on_load_click)\nembed_btn.on_click(on_embed_click)\n\n# Display\ndisplay(HTML(\"<h3>üîê HuggingFace Login</h3>\"))\ndisplay(widgets.HBox([token_input, model_dropdown]))\ndisplay(widgets.HBox([load_btn, embed_btn]))\ndisplay(progress)\ndisplay(embed_output)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "if embedding_results is not None:\n",
    "    save_embeddings(embedding_results, \"embeddings.pt\")\n",
    "    print(\"‚úÖ Saved embeddings.pt\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Generate embeddings first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Entropy Analysis\n",
    "\n",
    "Calculate Shannon entropy to identify conserved and variable positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: ENTROPY ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "from analysis.entropy_lib import analyze_entropy, entropy_summary\n",
    "\n",
    "# Load embeddings if needed\n",
    "if embedding_results is None:\n",
    "    if Path(\"embeddings.pt\").exists():\n",
    "        embedding_results = torch.load(\"embeddings.pt\", weights_only=False)\n",
    "        print(\"‚úÖ Loaded embeddings.pt\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Run Step 2 first or upload embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run entropy analysis - one sequence at a time\nall_entropy_results = []\n\ndef run_entropy_for_sequence(seq_id, logits):\n    \"\"\"Run entropy analysis on a single protein sequence.\"\"\"\n    single_result = analyze_entropy(\n        {\"sequence_id\": [seq_id], \"logits\": [logits]},\n        base=\"e\",\n        constrained_percentile=10.0,\n        flexible_percentile=90.0\n    )\n    return single_result\n\nif embedding_results is not None:\n    print(\"üîÑ Calculating entropy per sequence...\")\n\n    for seq_id, logits in zip(embedding_results[\"sequence_id\"], embedding_results[\"logits\"]):\n        if logits is None:\n            continue\n        result = run_entropy_for_sequence(seq_id, logits)\n        all_entropy_results.append(result)\n        print(f\"   ‚úÖ {seq_id}: mean entropy = {result['mean_entropy'][0]:.3f}\")\n\n    print(f\"\\n‚úÖ Analyzed {len(all_entropy_results)} sequences\")\n    for r in all_entropy_results:\n        print(f\"   ‚Ä¢ {r['sequence_id'][0]}: {r['num_residues'][0]} residues, \"\n              f\"mean H = {r['mean_entropy'][0]:.3f}, \"\n              f\"constrained = {len(r['constrained_positions'][0])}, \"\n              f\"flexible = {len(r['flexible_positions'][0])}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize entropy distribution for each sequence\nif all_entropy_results:\n    import matplotlib.pyplot as plt\n\n    for result in all_entropy_results:\n        entropy_vals = result[\"entropy\"][0].float().numpy()\n        seq_id = result[\"sequence_id\"][0]\n\n        fig, ax = plt.subplots(figsize=(12, 4))\n        ax.plot(entropy_vals, alpha=0.7)\n        ax.set_xlabel(\"Residue Position\")\n        ax.set_ylabel(\"Entropy (nats)\")\n        ax.set_title(f\"Entropy Profile: {seq_id}\")\n\n        # Mark constrained and flexible regions\n        constrained = result[\"constrained_positions\"][0].long().numpy()\n        flexible = result[\"flexible_positions\"][0].long().numpy()\n\n        ax.scatter(constrained, entropy_vals[constrained], c=\"blue\", s=10, alpha=0.5, label=\"Constrained\")\n        ax.scatter(flexible, entropy_vals[flexible], c=\"red\", s=10, alpha=0.5, label=\"Flexible\")\n        ax.legend()\n\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Logits Analysis\n",
    "\n",
    "Analyze amino acid propensities at specific positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 4: LOGITS ANALYSIS\n# ============================================================\n\nfrom analysis.logits_lib import analyze_residues, plot_heatmap, AA_VOCAB\n\ndef run_logits_for_sequence(seq_id, logits):\n    \"\"\"Run logits analysis on a single protein sequence, analyzing every residue.\"\"\"\n    seq_length = logits.shape[0]\n    residues_of_interest = {i: f\"Position {i+1}\" for i in range(seq_length)}\n\n    single_result = analyze_residues(\n        {\"sequence_id\": [seq_id], \"logits\": [logits]},\n        residues_of_interest=residues_of_interest,\n        pool_method=\"mean\",\n        scale_method=\"minmax\"\n    )\n    return single_result"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze logits - one sequence at a time\nall_logits_analyses = []\n\nif embedding_results is not None:\n    print(\"üîÑ Analyzing logits per sequence...\")\n\n    for seq_id, logits in zip(embedding_results[\"sequence_id\"], embedding_results[\"logits\"]):\n        if logits is None:\n            continue\n        analysis = run_logits_for_sequence(seq_id, logits)\n        all_logits_analyses.append({\"seq_id\": seq_id, \"analysis\": analysis})\n        print(f\"   ‚úÖ {seq_id}: {len(analysis['residue_labels'])} residues analyzed\")\n\n    print(f\"\\n‚úÖ Logits analysis complete for {len(all_logits_analyses)} sequences\")\n\n    # Display probabilities for each sequence\n    for item in all_logits_analyses:\n        print(f\"\\nüìã Amino acid probabilities: {item['seq_id']}\")\n        display(item[\"analysis\"][\"probs\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate heatmap for each sequence\nif all_logits_analyses:\n    for item in all_logits_analyses:\n        analysis = item[\"analysis\"]\n        plot_heatmap(\n            analysis[\"probs\"],\n            row_labels=analysis[\"residue_labels\"],\n            col_labels=AA_VOCAB,\n            title=f\"Amino Acid Propensity Heatmap: {item['seq_id']}\",\n            figsize=(12, 5),\n            cmap=\"coolwarm\"\n        )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Export Results\n",
    "\n",
    "Save all analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 5: EXPORT RESULTS\n# ============================================================\n\nfrom analysis.entropy_lib import save_entropy_results, entropy_summary\nfrom analysis.logits_lib import save_analysis\n\noutput_dir = Path(\"results\")\noutput_dir.mkdir(exist_ok=True)\n\n# Save entropy results (one CSV per sequence)\nif all_entropy_results:\n    for r in all_entropy_results:\n        seq_id = r[\"sequence_id\"][0]\n        df = entropy_summary(r)\n        df.to_csv(output_dir / f\"entropy_{seq_id}.csv\", index=False)\n    print(f\"‚úÖ Saved {len(all_entropy_results)} entropy CSVs to results/\")\n\n# Save logits analysis (one CSV per sequence)\nif all_logits_analyses:\n    for item in all_logits_analyses:\n        seq_id = item[\"seq_id\"]\n        save_analysis(item[\"analysis\"], str(output_dir / f\"logits_{seq_id}.csv\"))\n    print(f\"‚úÖ Saved {len(all_logits_analyses)} logits CSVs to results/\")\n\n# Save embeddings\nif embedding_results is not None:\n    save_embeddings(embedding_results, str(output_dir / \"embeddings.pt\"))\n    print(\"‚úÖ Saved results/embeddings.pt\")\n\nprint(\"\\nüéâ All results saved!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results (Colab only)\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    \n",
    "    # Create zip of results\n",
    "    shutil.make_archive(\"results\", \"zip\", \"results\")\n",
    "    colab_files.download(\"results.zip\")\n",
    "    print(\"üì• Downloading results.zip...\")\n",
    "else:\n",
    "    print(f\"üìÅ Results saved to: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Pipeline Summary\n",
    "\n",
    "This notebook orchestrates the complete protein analysis workflow:\n",
    "\n",
    "| Step | Library | Input | Output |\n",
    "|------|---------|-------|--------|\n",
    "| 1. FASTA Cleaning | `embedding.fasta_cleaner` | FASTA files | `sequences.csv`, `metadata.csv` |\n",
    "| 2. Embedding | `embedding.esmc_embed_lib` | `sequences.csv` | `embeddings.pt` |\n",
    "| 3. Entropy | `analysis.entropy_lib` | `embeddings.pt` | `entropy_summary.csv` |\n",
    "| 4. Logits | `analysis.logits_lib` | `embeddings.pt` | `logits_analysis.csv`, heatmaps |\n",
    "\n",
    "### Using the libraries directly:\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "from embedding import process_fasta_files, load_esmc_model, embed_from_csv\n",
    "from analysis import analyze_entropy, analyze_residues\n",
    "\n",
    "# Process FASTA\n",
    "seq_df, meta_df = process_fasta_files(\"proteins.fasta\")\n",
    "\n",
    "# Generate embeddings\n",
    "model = load_esmc_model(\"hf_token\")\n",
    "results = embed_from_csv(model, \"sequences.csv\")\n",
    "\n",
    "# Analyze\n",
    "entropy = analyze_entropy(results)\n",
    "logits = analyze_residues(results, residues_of_interest={100: \"D100\"})\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
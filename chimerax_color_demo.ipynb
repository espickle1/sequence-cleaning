{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/espickle1/sequence-cleaning/blob/main/chimerax_color_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueXiD7-9aeQl"
   },
   "source": [
    "# ChimeraX Color Script Generator\n",
    "\n",
    "Upload per-residue entropy (or any scalar) values, sequences, and metadata.\n",
    "This notebook generates a `.cxc` ChimeraX color script **per sequence** and\n",
    "downloads them to your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCVVA1A6aeQm"
   },
   "source": [
    "## 0. Setup – Clone repo & install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ky2AL_smaeQm",
    "outputId": "53d55561-070a-4306-d86d-6d9a53f6b0cc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"sequence-cleaning\"\n",
    "if not os.path.isdir(repo_dir):\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/espickle1/sequence-cleaning.git\"],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "if os.path.basename(os.getcwd()) != repo_dir:\n",
    "    os.chdir(repo_dir)\n",
    "    \n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IshNBpejaeQn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "from analysis.chimerax_color_lib import (\n",
    "    generate_chimerax_script,\n",
    "    generate_chimerax_script_with_scalers,\n",
    "    write_chimerax_script,\n",
    "    fit_scaler,\n",
    "    fit_minmax_scaler,\n",
    "    scale_values_with_scaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv0sRFnZaeQn"
   },
   "source": [
    "## 1. Upload files\n",
    "\n",
    "Upload three CSV files:\n",
    "- **Metadata file** – must contain `sequence_id` and `name` columns.\n",
    "- **Sequences file** – must contain `sequence_id` and `sequence` columns.\n",
    "- **Entropy file** – per-residue entropy values. Expected columns: `sequence_id` plus one or more value columns, **or** a single per-residue file (e.g. `residue_position, entropy, ...`) that applies to one sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 742
    },
    "id": "OP6vrFPuaeQn",
    "outputId": "8a7af67d-c2e2-4dd9-e761-60da5776a75b"
   },
   "outputs": [],
   "source": [
    "print(\"Upload the METADATA file (.csv):\")\n",
    "meta_upload = files.upload()\n",
    "meta_filename = list(meta_upload.keys())[0]\n",
    "df_metadata = pd.read_csv(meta_filename)\n",
    "print(f\"Loaded {meta_filename}: {df_metadata.shape}\")\n",
    "print(f\"Columns: {list(df_metadata.columns)}\")\n",
    "display(df_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "4oZ7WpXmcYs6",
    "outputId": "2126cc29-cb79-4519-be25-421f5b475b64"
   },
   "outputs": [],
   "source": [
    "print(\"Upload the SEQUENCES file (.csv):\")\n",
    "seq_upload = files.upload()\n",
    "seq_filename = list(seq_upload.keys())[0]\n",
    "df_sequences = pd.read_csv(seq_filename)\n",
    "print(f\"Loaded {seq_filename}: {df_sequences.shape}\")\n",
    "print(f\"Columns: {list(df_sequences.columns)}\")\n",
    "display(df_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "4EEsVuadcWIB",
    "outputId": "6b56bb8b-8989-4ac2-a4da-b784507be85c"
   },
   "outputs": [],
   "source": [
    "print(\"Upload the ENTROPY file (.csv):\")\n",
    "entropy_upload = files.upload()\n",
    "entropy_filename = list(entropy_upload.keys())[0]\n",
    "df_entropy = pd.read_csv(entropy_filename)\n",
    "print(f\"Loaded {entropy_filename}: {df_entropy.shape}\")\n",
    "print(f\"Columns: {list(df_entropy.columns)}\")\n",
    "df_entropy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Multi-file normalization (optional)\n",
    "\n",
    "Enable this to normalize entropy values across **multiple** entropy files. This ensures\n",
    "all sequences use the same color scale, making comparisons meaningful.\n",
    "\n",
    "- **Single file mode** (default): Each sequence is colored based on its own min/max values.\n",
    "- **Multi-file mode**: All entropy values are combined to compute a shared scale, then each\n",
    "  sequence is colored using that shared scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Multi-file normalization setup ---\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "_multi = input(\"Use multi-file normalization? (y/n) [n]: \").strip().lower()\n",
    "MULTI_FILE_MODE = _multi in (\"y\", \"yes\")\n",
    "\n",
    "# Store all entropy data: list of dicts with 'seq_id', 'label', 'values', 'filename', 'model', 'chain'\n",
    "all_entropy_data = []\n",
    "\n",
    "if MULTI_FILE_MODE:\n",
    "    print(\"\\n--- Multi-file mode enabled ---\")\n",
    "    print(\"Upload additional entropy files. These will be combined with the first file\")\n",
    "    print(\"to compute a shared normalization scale.\")\n",
    "    print(\"You will specify model/chain IDs for each file.\\n\")\n",
    "    \n",
    "    # Add the first file that was already uploaded\n",
    "    if \"sequence_id\" not in df_entropy.columns:\n",
    "        # Per-residue format - extract seq_id from filename\n",
    "        match = re.search(r\"entropy_per_residue_(.+)\\.csv\", entropy_filename)\n",
    "        first_seq_id = match.group(1) if match else Path(entropy_filename).stem\n",
    "        \n",
    "        # Get value column\n",
    "        if \"entropy\" in df_entropy.columns:\n",
    "            value_col = \"entropy\"\n",
    "        else:\n",
    "            numeric_cols = [c for c in df_entropy.select_dtypes(include=\"number\").columns if c != \"residue_position\"]\n",
    "            value_col = numeric_cols[0] if numeric_cols else df_entropy.columns[-1]\n",
    "        \n",
    "        # Look up name from metadata\n",
    "        first_label = first_seq_id\n",
    "        if \"name\" in df_metadata.columns and \"sequence_id\" in df_metadata.columns:\n",
    "            name_match = df_metadata.loc[df_metadata[\"sequence_id\"] == first_seq_id, \"name\"]\n",
    "            if len(name_match) and pd.notna(name_match.iloc[0]):\n",
    "                first_label = name_match.iloc[0]\n",
    "        \n",
    "        # Get model/chain for first file\n",
    "        print(f\"File 1: {entropy_filename} -> {first_label}\")\n",
    "        _model = input(f\"  Model ID for '{first_label}' [1]: \").strip() or \"1\"\n",
    "        _chain = input(f\"  Chain ID for '{first_label}' (blank for none): \").strip()\n",
    "        \n",
    "        all_entropy_data.append({\n",
    "            \"seq_id\": first_seq_id,\n",
    "            \"label\": first_label,\n",
    "            \"values\": df_entropy[value_col].values.astype(float),\n",
    "            \"filename\": entropy_filename,\n",
    "            \"model\": int(_model),\n",
    "            \"chain\": _chain,\n",
    "        })\n",
    "        print(f\"  -> Model #{_model}\" + (f\"/{_chain}\" if _chain else \"\") + f\", {len(df_entropy)} residues\\n\")\n",
    "    else:\n",
    "        raise ValueError(\"Multi-file mode currently only supports per-residue format (Format B) entropy files.\")\n",
    "    \n",
    "    # Upload additional files\n",
    "    while True:\n",
    "        _more = input(\"Upload another entropy file? (y/n) [n]: \").strip().lower()\n",
    "        if _more not in (\"y\", \"yes\"):\n",
    "            break\n",
    "        \n",
    "        print(\"\\nUpload the next ENTROPY file (.csv):\")\n",
    "        extra_upload = files.upload()\n",
    "        extra_filename = list(extra_upload.keys())[0]\n",
    "        df_extra = pd.read_csv(extra_filename)\n",
    "        \n",
    "        if \"sequence_id\" in df_extra.columns:\n",
    "            print(\"Warning: File has sequence_id column. Multi-file mode expects per-residue files.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract seq_id from filename\n",
    "        match = re.search(r\"entropy_per_residue_(.+)\\.csv\", extra_filename)\n",
    "        extra_seq_id = match.group(1) if match else Path(extra_filename).stem\n",
    "        \n",
    "        # Get value column\n",
    "        if \"entropy\" in df_extra.columns:\n",
    "            extra_value_col = \"entropy\"\n",
    "        else:\n",
    "            numeric_cols = [c for c in df_extra.select_dtypes(include=\"number\").columns if c != \"residue_position\"]\n",
    "            extra_value_col = numeric_cols[0] if numeric_cols else df_extra.columns[-1]\n",
    "        \n",
    "        # Look up name\n",
    "        extra_label = extra_seq_id\n",
    "        if \"name\" in df_metadata.columns and \"sequence_id\" in df_metadata.columns:\n",
    "            name_match = df_metadata.loc[df_metadata[\"sequence_id\"] == extra_seq_id, \"name\"]\n",
    "            if len(name_match) and pd.notna(name_match.iloc[0]):\n",
    "                extra_label = name_match.iloc[0]\n",
    "        \n",
    "        # Get model/chain for this file\n",
    "        file_num = len(all_entropy_data) + 1\n",
    "        print(f\"File {file_num}: {extra_filename} -> {extra_label}\")\n",
    "        _model = input(f\"  Model ID for '{extra_label}' [{file_num}]: \").strip() or str(file_num)\n",
    "        _chain = input(f\"  Chain ID for '{extra_label}' (blank for none): \").strip()\n",
    "        \n",
    "        all_entropy_data.append({\n",
    "            \"seq_id\": extra_seq_id,\n",
    "            \"label\": extra_label,\n",
    "            \"values\": df_extra[extra_value_col].values.astype(float),\n",
    "            \"filename\": extra_filename,\n",
    "            \"model\": int(_model),\n",
    "            \"chain\": _chain,\n",
    "        })\n",
    "        print(f\"  -> Model #{_model}\" + (f\"/{_chain}\" if _chain else \"\") + f\", {len(df_extra)} residues\\n\")\n",
    "    \n",
    "    print(f\"\\n=== Loaded {len(all_entropy_data)} entropy file(s) for multi-file normalization ===\")\n",
    "    for i, data in enumerate(all_entropy_data, 1):\n",
    "        spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n",
    "        print(f\"  {i}. {data['label']} ({data['seq_id']}): {len(data['values'])} residues, {spec}\")\n",
    "else:\n",
    "    print(\"Single-file mode: each sequence normalized independently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO9cxKemaeQn"
   },
   "source": [
    "## 2. Merge files on `sequence_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "iWiT1xcnaeQn",
    "outputId": "8abe9fd0-29b8-46f3-d2d6-60c764c3e13d"
   },
   "outputs": [],
   "source": [
    "# Detect entropy file format and build a unified structure\n",
    "#\n",
    "# Format A: Wide table with sequence_id + value columns (one row per sequence)\n",
    "# Format B: Per-residue table (residue_position, entropy, ...) for a single sequence\n",
    "#           In this case sequence_id is extracted from the filename.\n",
    "\n",
    "if \"sequence_id\" in df_entropy.columns:\n",
    "    # Format A -- entropy file already has sequence_id\n",
    "    available_ids = df_entropy[\"sequence_id\"].unique().tolist()\n",
    "    print(f\"Available sequence_ids ({len(available_ids)}):\")\n",
    "    for sid in available_ids:\n",
    "        print(f\"  - {sid}\")\n",
    "\n",
    "    print(\"\\nEnter sequence_id(s) to process (comma-separated), or 'all' for all:\")\n",
    "    selection = input(\"sequence_id(s) [all]: \").strip() or \"all\"\n",
    "\n",
    "    if selection.lower() == \"all\":\n",
    "        selected_ids = available_ids\n",
    "    else:\n",
    "        selected_ids = [s.strip() for s in selection.split(\",\") if s.strip()]\n",
    "        # Validate selections\n",
    "        invalid = [s for s in selected_ids if s not in available_ids]\n",
    "        if invalid:\n",
    "            print(f\"Warning: unknown sequence_id(s) ignored: {invalid}\")\n",
    "        selected_ids = [s for s in selected_ids if s in available_ids]\n",
    "\n",
    "    if not selected_ids:\n",
    "        raise ValueError(\"No valid sequence_id selected.\")\n",
    "\n",
    "    print(f\"\\nSelected {len(selected_ids)} sequence(s): {selected_ids}\")\n",
    "\n",
    "    # Filter entropy data to selected IDs\n",
    "    df_entropy_filtered = df_entropy[df_entropy[\"sequence_id\"].isin(selected_ids)]\n",
    "\n",
    "    merge_cols = [\"sequence_id\", \"sequence\"]\n",
    "    if \"sequence\" in df_sequences.columns:\n",
    "        df_merged = df_entropy_filtered.merge(\n",
    "            df_sequences[merge_cols], on=\"sequence_id\", how=\"inner\"\n",
    "        )\n",
    "    else:\n",
    "        df_merged = df_entropy_filtered.copy()\n",
    "\n",
    "    if \"name\" in df_metadata.columns:\n",
    "        df_merged = df_merged.merge(\n",
    "            df_metadata[[\"sequence_id\", \"name\"]], on=\"sequence_id\", how=\"left\"\n",
    "        )\n",
    "\n",
    "    value_columns = [\n",
    "        c for c in df_entropy.columns if c != \"sequence_id\"\n",
    "    ]\n",
    "    print(f\"Format A detected (wide table). Value columns: {value_columns}\")\n",
    "    print(f\"Merged rows: {len(df_merged)}\")\n",
    "    display(df_merged.head())\n",
    "    ENTROPY_FORMAT = \"wide\"\n",
    "\n",
    "else:\n",
    "    # Format B -- per-residue file without sequence_id\n",
    "    # Try to extract sequence_id from the entropy filename\n",
    "    # Pipeline exports files named like: entropy_per_residue_{seq_id}.csv\n",
    "    import re\n",
    "    match = re.search(r\"entropy_per_residue_(.+)\\.csv\", entropy_filename)\n",
    "    if match:\n",
    "        inferred_seq_id = match.group(1)\n",
    "    else:\n",
    "        inferred_seq_id = Path(entropy_filename).stem\n",
    "\n",
    "    print(f\"Format B detected (per-residue). Inferred sequence_id: {inferred_seq_id}\")\n",
    "    override = input(\"Use this sequence_id? (press Enter to accept, or type a new one): \").strip()\n",
    "    if override:\n",
    "        inferred_seq_id = override\n",
    "        print(f\"Using sequence_id: {inferred_seq_id}\")\n",
    "\n",
    "    # Determine which column holds the values\n",
    "    if \"entropy\" in df_entropy.columns:\n",
    "        value_col = \"entropy\"\n",
    "    else:\n",
    "        # Use the first numeric column that isn't residue_position\n",
    "        numeric_cols = [\n",
    "            c for c in df_entropy.select_dtypes(include=\"number\").columns\n",
    "            if c != \"residue_position\"\n",
    "        ]\n",
    "        value_col = numeric_cols[0] if numeric_cols else df_entropy.columns[-1]\n",
    "\n",
    "    print(f\"Using value column: '{value_col}'\")\n",
    "    print(f\"Residues: {len(df_entropy)}\")\n",
    "\n",
    "    # Look up the name from metadata\n",
    "    label = inferred_seq_id\n",
    "    if \"name\" in df_metadata.columns and \"sequence_id\" in df_metadata.columns:\n",
    "        name_match = df_metadata.loc[\n",
    "            df_metadata[\"sequence_id\"] == inferred_seq_id, \"name\"\n",
    "        ]\n",
    "        if len(name_match):\n",
    "            label = name_match.iloc[0]\n",
    "\n",
    "    # Store for the generation step\n",
    "    df_merged = None\n",
    "    per_residue_info = {\n",
    "        \"seq_id\": inferred_seq_id,\n",
    "        \"label\": label,\n",
    "        \"values\": df_entropy[value_col].values.astype(float),\n",
    "    }\n",
    "    value_columns = None\n",
    "    ENTROPY_FORMAT = \"per_residue\"\n",
    "    display(df_entropy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LXqXapPaeQo"
   },
   "source": [
    "## 3. Configure color mapping\n",
    "\n",
    "Adjust these parameters as needed before generating the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhRoMu5RaeQo",
    "outputId": "e2e621a0-3663-40a8-a9de-377949d719ee"
   },
   "outputs": [],
   "source": [
    "# --- Interactive Configuration ---\n",
    "\n",
    "# Shared settings (apply to all files)\n",
    "if ENTROPY_FORMAT == \"wide\" and value_columns:\n",
    "    print(f\"Value columns: {value_columns}\")\n",
    "\n",
    "CMAP_NAME = input(\"Colormap name [Greys]: \").strip() or \"Greys\"\n",
    "\n",
    "print(\"Transform methods: none, log, minmax, quantile, power, standard, robust\")\n",
    "TRANSFORM_METHOD = input(\"Transform method [none]: \").strip() or \"none\"\n",
    "\n",
    "_color = input(\"Enable color mapping? (y/n) [y]: \").strip().lower()\n",
    "COLOR = _color not in (\"n\", \"no\")\n",
    "\n",
    "COLOR_INVERT = False\n",
    "if COLOR:\n",
    "    _ci = input(\"Invert colormap? (y/n) [n]: \").strip().lower()\n",
    "    COLOR_INVERT = _ci in (\"y\", \"yes\")\n",
    "\n",
    "_trans = input(\"Enable transparency mapping? (y/n) [n]: \").strip().lower()\n",
    "TRANSPARENCY = _trans in (\"y\", \"yes\")\n",
    "\n",
    "TRANSPARENCY_INVERT = False\n",
    "if TRANSPARENCY:\n",
    "    _ti = input(\"Invert transparency? (y/n) [n]: \").strip().lower()\n",
    "    TRANSPARENCY_INVERT = _ti in (\"y\", \"yes\")\n",
    "\n",
    "# Model/chain configuration depends on mode\n",
    "if MULTI_FILE_MODE and all_entropy_data:\n",
    "    # Multi-file mode: model/chain already set per file, allow editing\n",
    "    print(\"\\n--- Per-file Model/Chain Configuration ---\")\n",
    "    print(\"Current assignments:\")\n",
    "    for i, data in enumerate(all_entropy_data):\n",
    "        spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n",
    "        print(f\"  {i+1}. {data['label']}: {spec}\")\n",
    "    \n",
    "    _edit = input(\"\\nEdit model/chain assignments? (y/n) [n]: \").strip().lower()\n",
    "    if _edit in (\"y\", \"yes\"):\n",
    "        for i, data in enumerate(all_entropy_data):\n",
    "            print(f\"\\n{data['label']} (currently #{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\") + \"):\")\n",
    "            _model = input(f\"  Model ID [{data['model']}]: \").strip()\n",
    "            if _model:\n",
    "                data['model'] = int(_model)\n",
    "            _chain = input(f\"  Chain ID [{data['chain'] or 'none'}]: \").strip()\n",
    "            if _chain.lower() == 'none':\n",
    "                data['chain'] = \"\"\n",
    "            elif _chain:\n",
    "                data['chain'] = _chain\n",
    "        \n",
    "        print(\"\\nUpdated assignments:\")\n",
    "        for i, data in enumerate(all_entropy_data):\n",
    "            spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n",
    "            print(f\"  {i+1}. {data['label']}: {spec}\")\n",
    "    \n",
    "    # These are not used in multi-file mode but define for consistency\n",
    "    MODEL = None\n",
    "    CHAIN = None\n",
    "else:\n",
    "    # Single-file mode: one model/chain for all\n",
    "    _model = input(\"Model ID [1]: \").strip() or \"1\"\n",
    "    MODEL = int(_model)\n",
    "    CHAIN = input(\"Chain ID (e.g. A, B — leave blank for none): \").strip()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== Configuration Summary ===\")\n",
    "print(f\"Colormap: {CMAP_NAME}, Transform: {TRANSFORM_METHOD}\")\n",
    "print(f\"Color: {COLOR} (invert={COLOR_INVERT}), Transparency: {TRANSPARENCY} (invert={TRANSPARENCY_INVERT})\")\n",
    "\n",
    "if MULTI_FILE_MODE and all_entropy_data:\n",
    "    print(\"Mode: Multi-file normalization\")\n",
    "    for data in all_entropy_data:\n",
    "        spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n",
    "        print(f\"  - {data['label']}: {spec}\")\n",
    "else:\n",
    "    print(\"Mode: Single-file\")\n",
    "    print(f\"Model: #{MODEL}\" + (f\"/{CHAIN}\" if CHAIN else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYrwjdYFaeQo"
   },
   "source": [
    "## 4. Generate `.cxc` scripts (one per sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVaYSPPjaeQo",
    "outputId": "3f820b88-b86d-447c-c299-ca0d0e5ce0e9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = \"cxc_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "generated_files = []\n",
    "\n",
    "def _make_cxc(values, label, seq_id, model, chain):\n",
    "    \"\"\"Generate a .cxc file for one sequence (single-file mode).\"\"\"\n",
    "    script = generate_chimerax_script(\n",
    "        values,\n",
    "        cmap_name=CMAP_NAME,\n",
    "        transform_method=TRANSFORM_METHOD,\n",
    "        color=COLOR,\n",
    "        color_invert=COLOR_INVERT,\n",
    "        transparency=TRANSPARENCY,\n",
    "        transparency_invert=TRANSPARENCY_INVERT,\n",
    "        model=model,\n",
    "        chain=chain,\n",
    "    )\n",
    "    safe_label = str(label).replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    chain_str = chain if chain else \"\"\n",
    "    cmap_str = f\"{CMAP_NAME}_i\" if COLOR_INVERT else CMAP_NAME\n",
    "    out_path = os.path.join(output_dir, f\"{safe_label}_{seq_id}_{model}{chain_str}_{cmap_str}_{TRANSFORM_METHOD}.cxc\")\n",
    "    write_chimerax_script(script, out_path)\n",
    "    return out_path\n",
    "\n",
    "def _make_cxc_with_scalers(values, label, seq_id, model, chain, transform_scaler, minmax_scaler):\n",
    "    \"\"\"Generate a .cxc file using pre-fitted scalers (multi-file mode).\"\"\"\n",
    "    script = generate_chimerax_script_with_scalers(\n",
    "        values,\n",
    "        transform_scaler=transform_scaler,\n",
    "        minmax_scaler=minmax_scaler,\n",
    "        cmap_name=CMAP_NAME,\n",
    "        color=COLOR,\n",
    "        color_invert=COLOR_INVERT,\n",
    "        transparency=TRANSPARENCY,\n",
    "        transparency_invert=TRANSPARENCY_INVERT,\n",
    "        model=model,\n",
    "        chain=chain,\n",
    "    )\n",
    "    safe_label = str(label).replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    chain_str = chain if chain else \"\"\n",
    "    cmap_str = f\"{CMAP_NAME}_i\" if COLOR_INVERT else CMAP_NAME\n",
    "    out_path = os.path.join(output_dir, f\"{safe_label}_{seq_id}_{model}{chain_str}_{cmap_str}_{TRANSFORM_METHOD}_multifile.cxc\")\n",
    "    write_chimerax_script(script, out_path)\n",
    "    return out_path\n",
    "\n",
    "# --- Multi-file mode: fit scalers on combined data ---\n",
    "if MULTI_FILE_MODE and all_entropy_data:\n",
    "    print(\"=== Multi-file normalization ===\")\n",
    "    \n",
    "    # Combine all values\n",
    "    combined_values = np.concatenate([d[\"values\"] for d in all_entropy_data])\n",
    "    print(f\"Combined {len(all_entropy_data)} files: {len(combined_values)} total residues\")\n",
    "    print(f\"  Min: {combined_values.min():.4f}, Max: {combined_values.max():.4f}\")\n",
    "    print(f\"  Mean: {combined_values.mean():.4f}, Std: {combined_values.std():.4f}\")\n",
    "    \n",
    "    # Fit transform scaler on combined data\n",
    "    transform_scaler = fit_scaler(combined_values, method=TRANSFORM_METHOD)\n",
    "    \n",
    "    # Apply transform to combined data, then fit minmax scaler\n",
    "    if transform_scaler is not None:\n",
    "        scaled_combined = scale_values_with_scaler(combined_values, transform_scaler)\n",
    "    else:\n",
    "        scaled_combined = combined_values\n",
    "    \n",
    "    minmax_scaler = fit_minmax_scaler(scaled_combined)\n",
    "    print(f\"Fitted scalers on combined data (transform={TRANSFORM_METHOD})\\n\")\n",
    "    \n",
    "    # Generate .cxc for each sequence using the shared scalers and per-file model/chain\n",
    "    for data in all_entropy_data:\n",
    "        out_path = _make_cxc_with_scalers(\n",
    "            data[\"values\"],\n",
    "            data[\"label\"],\n",
    "            data[\"seq_id\"],\n",
    "            data[\"model\"],\n",
    "            data[\"chain\"],\n",
    "            transform_scaler,\n",
    "            minmax_scaler,\n",
    "        )\n",
    "        generated_files.append(out_path)\n",
    "        spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n",
    "        print(f\"  Created: {out_path} ({spec})\")\n",
    "\n",
    "# --- Single-file mode ---\n",
    "elif ENTROPY_FORMAT == \"wide\":\n",
    "    # One row per sequence -- iterate over df_merged\n",
    "    for _, row in df_merged.iterrows():\n",
    "        seq_id = row[\"sequence_id\"]\n",
    "        label = row.get(\"name\", seq_id) or seq_id\n",
    "        values = row[value_columns].values.astype(float)\n",
    "\n",
    "        out_path = _make_cxc(values, label, seq_id, MODEL, CHAIN)\n",
    "        generated_files.append(out_path)\n",
    "        print(f\"  Created: {out_path}\")\n",
    "else:\n",
    "    # Per-residue format -- single sequence\n",
    "    out_path = _make_cxc(\n",
    "        per_residue_info[\"values\"],\n",
    "        per_residue_info[\"label\"],\n",
    "        per_residue_info[\"seq_id\"],\n",
    "        MODEL,\n",
    "        CHAIN,\n",
    "    )\n",
    "    generated_files.append(out_path)\n",
    "    print(f\"  Created: {out_path}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(generated_files)} .cxc file(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15_ECcEKaeQo"
   },
   "source": [
    "## 5. Download `.cxc` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "PKEKsbxJaeQo",
    "outputId": "ea8c67f4-121f-4443-cb47-e708212a45f1"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_22d60619-4f6a-4a14-b5d9-133489664103\", \"PB1_0ea58344488f_1a_Blues.cxc\", 40574)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in generated_files:\n",
    "    files.download(f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

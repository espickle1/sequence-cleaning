{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/espickle1/sequence-cleaning/blob/main/chimerax_color_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueXiD7-9aeQl"
   },
   "source": [
    "# ChimeraX Color Script Generator\n",
    "\n",
    "Upload per-residue entropy (or any scalar) values, sequences, and metadata.\n",
    "This notebook generates a `.cxc` ChimeraX color script **per sequence** and\n",
    "downloads them to your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCVVA1A6aeQm"
   },
   "source": [
    "## 0. Setup – Clone repo & install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ky2AL_smaeQm",
    "outputId": "53d55561-070a-4306-d86d-6d9a53f6b0cc"
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "repo_dir = \"sequence-cleaning\"\n",
    "if not os.path.isdir(repo_dir):\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/espickle1/sequence-cleaning.git\"],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "if os.path.basename(os.getcwd()) != repo_dir:\n",
    "    os.chdir(repo_dir)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IshNBpejaeQn"
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom google.colab import files\nfrom analysis.chimerax_color_lib import (\n    generate_chimerax_script,\n    generate_chimerax_script_with_scalers,\n    write_chimerax_script,\n    fit_scaler,\n    fit_minmax_scaler,\n    scale_values_with_scaler,\n    scale_values,\n    compute_value_statistics,\n    parse_range_string,\n    create_value_mask,\n    display_statistics_summary,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv0sRFnZaeQn"
   },
   "source": [
    "## 1. Upload files\n",
    "\n",
    "Upload three CSV files:\n",
    "- **Metadata file** – must contain `sequence_id` and `name` columns.\n",
    "- **Sequences file** – must contain `sequence_id` and `sequence` columns.\n",
    "- **Entropy file** – per-residue entropy values. Expected columns: `sequence_id` plus one or more value columns, **or** a single per-residue file (e.g. `residue_position, entropy, ...`) that applies to one sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 742
    },
    "id": "OP6vrFPuaeQn",
    "outputId": "8a7af67d-c2e2-4dd9-e761-60da5776a75b"
   },
   "outputs": [],
   "source": [
    "print(\"Upload the METADATA file (.csv):\")\n",
    "meta_upload = files.upload()\n",
    "meta_filename = list(meta_upload.keys())[0]\n",
    "df_metadata = pd.read_csv(meta_filename)\n",
    "print(f\"Loaded {meta_filename}: {df_metadata.shape}\")\n",
    "print(f\"Columns: {list(df_metadata.columns)}\")\n",
    "display(df_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "4oZ7WpXmcYs6",
    "outputId": "2126cc29-cb79-4519-be25-421f5b475b64"
   },
   "outputs": [],
   "source": [
    "print(\"Upload the SEQUENCES file (.csv):\")\n",
    "seq_upload = files.upload()\n",
    "seq_filename = list(seq_upload.keys())[0]\n",
    "df_sequences = pd.read_csv(seq_filename)\n",
    "print(f\"Loaded {seq_filename}: {df_sequences.shape}\")\n",
    "print(f\"Columns: {list(df_sequences.columns)}\")\n",
    "display(df_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "4EEsVuadcWIB",
    "outputId": "6b56bb8b-8989-4ac2-a4da-b784507be85c"
   },
   "outputs": [],
   "source": [
    "print(\"Upload the ENTROPY file (.csv):\")\n",
    "entropy_upload = files.upload()\n",
    "entropy_filename = list(entropy_upload.keys())[0]\n",
    "df_entropy = pd.read_csv(entropy_filename)\n",
    "print(f\"Loaded {entropy_filename}: {df_entropy.shape}\")\n",
    "print(f\"Columns: {list(df_entropy.columns)}\")\n",
    "df_entropy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Multi-file normalization (optional)\n",
    "\n",
    "Enable this to normalize entropy values across **multiple** entropy files. This ensures\n",
    "all sequences use the same color scale, making comparisons meaningful.\n",
    "\n",
    "- **Single file mode** (default): Each sequence is colored based on its own min/max values.\n",
    "- **Multi-file mode**: All entropy values are combined to compute a shared scale, then each\n",
    "  sequence is colored using that shared scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Multi-file normalization setup ---\nimport re\nfrom pathlib import Path\n\n_multi = input(\"Use multi-file normalization? (y/n) [n]: \").strip().lower()\nMULTI_FILE_MODE = _multi in (\"y\", \"yes\")\n\n# Store all entropy data: list of dicts with 'seq_id', 'label', 'values', 'filename', 'model', 'chain'\nall_entropy_data = []\n\nif MULTI_FILE_MODE:\n    print(\"\\n--- Multi-file mode enabled ---\")\n    print(\"Upload additional entropy files. These will be combined with the first file\")\n    print(\"to compute a shared normalization scale.\")\n    print(\"You will specify model/chain IDs for each file.\\n\")\n    \n    # Add the first file that was already uploaded\n    if \"sequence_id\" not in df_entropy.columns:\n        # Per-residue format - extract seq_id from filename\n        match = re.search(r\"entropy_per_residue_(.+)\\.csv\", entropy_filename)\n        first_seq_id = match.group(1) if match else Path(entropy_filename).stem\n        \n        # Get value column\n        if \"entropy\" in df_entropy.columns:\n            value_col = \"entropy\"\n        else:\n            numeric_cols = [c for c in df_entropy.select_dtypes(include=\"number\").columns if c != \"residue_position\"]\n            value_col = numeric_cols[0] if numeric_cols else df_entropy.columns[-1]\n        \n        # Look up name from metadata\n        first_label = first_seq_id\n        if \"name\" in df_metadata.columns and \"sequence_id\" in df_metadata.columns:\n            name_match = df_metadata.loc[df_metadata[\"sequence_id\"] == first_seq_id, \"name\"]\n            if len(name_match) and pd.notna(name_match.iloc[0]):\n                first_label = name_match.iloc[0]\n        \n        # Get model/chain for first file\n        print(f\"File 1: {entropy_filename} -> {first_label}\")\n        _model = input(f\"  Model ID for '{first_label}' [1]: \").strip() or \"1\"\n        _chain = input(f\"  Chain ID for '{first_label}' (blank for none): \").strip()\n        \n        all_entropy_data.append({\n            \"seq_id\": first_seq_id,\n            \"label\": first_label,\n            \"values\": df_entropy[value_col].values.astype(float),\n            \"filename\": entropy_filename,\n            \"model\": int(_model),\n            \"chain\": _chain,\n        })\n        print(f\"  -> Model #{_model}\" + (f\"/{_chain}\" if _chain else \"\") + f\", {len(df_entropy)} residues\\n\")\n    else:\n        raise ValueError(\"Multi-file mode currently only supports per-residue format (Format B) entropy files.\")\n    \n    # Upload additional files\n    while True:\n        _more = input(\"Upload another entropy file? (y/n) [n]: \").strip().lower()\n        if _more not in (\"y\", \"yes\"):\n            break\n        \n        print(\"\\nUpload the next ENTROPY file (.csv):\")\n        extra_upload = files.upload()\n        extra_filename = list(extra_upload.keys())[0]\n        df_extra = pd.read_csv(extra_filename)\n        \n        if \"sequence_id\" in df_extra.columns:\n            print(\"Warning: File has sequence_id column. Multi-file mode expects per-residue files.\")\n            continue\n        \n        # Extract seq_id from filename\n        match = re.search(r\"entropy_per_residue_(.+)\\.csv\", extra_filename)\n        extra_seq_id = match.group(1) if match else Path(extra_filename).stem\n        \n        # Get value column\n        if \"entropy\" in df_extra.columns:\n            extra_value_col = \"entropy\"\n        else:\n            numeric_cols = [c for c in df_extra.select_dtypes(include=\"number\").columns if c != \"residue_position\"]\n            extra_value_col = numeric_cols[0] if numeric_cols else df_extra.columns[-1]\n        \n        # Look up name\n        extra_label = extra_seq_id\n        if \"name\" in df_metadata.columns and \"sequence_id\" in df_metadata.columns:\n            name_match = df_metadata.loc[df_metadata[\"sequence_id\"] == extra_seq_id, \"name\"]\n            if len(name_match) and pd.notna(name_match.iloc[0]):\n                extra_label = name_match.iloc[0]\n        \n        # Get model/chain for this file\n        file_num = len(all_entropy_data) + 1\n        print(f\"File {file_num}: {extra_filename} -> {extra_label}\")\n        _model = input(f\"  Model ID for '{extra_label}' [{file_num}]: \").strip() or str(file_num)\n        _chain = input(f\"  Chain ID for '{extra_label}' (blank for none): \").strip()\n        \n        all_entropy_data.append({\n            \"seq_id\": extra_seq_id,\n            \"label\": extra_label,\n            \"values\": df_extra[extra_value_col].values.astype(float),\n            \"filename\": extra_filename,\n            \"model\": int(_model),\n            \"chain\": _chain,\n        })\n        print(f\"  -> Model #{_model}\" + (f\"/{_chain}\" if _chain else \"\") + f\", {len(df_extra)} residues\\n\")\n    \n    print(f\"\\n=== Loaded {len(all_entropy_data)} entropy file(s) for multi-file normalization ===\")\n    for i, data in enumerate(all_entropy_data, 1):\n        spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n        print(f\"  {i}. {data['label']} ({data['seq_id']}): {len(data['values'])} residues, {spec}\")\nelse:\n    print(\"Single-file mode: each sequence normalized independently.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO9cxKemaeQn"
   },
   "source": [
    "## 2. Merge files on `sequence_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "iWiT1xcnaeQn",
    "outputId": "8abe9fd0-29b8-46f3-d2d6-60c764c3e13d"
   },
   "outputs": [],
   "source": [
    "# Detect entropy file format and build a unified structure\n",
    "#\n",
    "# Format A: Wide table with sequence_id + value columns (one row per sequence)\n",
    "# Format B: Per-residue table (residue_position, entropy, ...) for a single sequence\n",
    "#           In this case sequence_id is extracted from the filename.\n",
    "\n",
    "if \"sequence_id\" in df_entropy.columns:\n",
    "    # Format A -- entropy file already has sequence_id\n",
    "    available_ids = df_entropy[\"sequence_id\"].unique().tolist()\n",
    "    print(f\"Available sequence_ids ({len(available_ids)}):\")\n",
    "    for sid in available_ids:\n",
    "        print(f\"  - {sid}\")\n",
    "\n",
    "    print(\"\\nEnter sequence_id(s) to process (comma-separated), or 'all' for all:\")\n",
    "    selection = input(\"sequence_id(s) [all]: \").strip() or \"all\"\n",
    "\n",
    "    if selection.lower() == \"all\":\n",
    "        selected_ids = available_ids\n",
    "    else:\n",
    "        selected_ids = [s.strip() for s in selection.split(\",\") if s.strip()]\n",
    "        # Validate selections\n",
    "        invalid = [s for s in selected_ids if s not in available_ids]\n",
    "        if invalid:\n",
    "            print(f\"Warning: unknown sequence_id(s) ignored: {invalid}\")\n",
    "        selected_ids = [s for s in selected_ids if s in available_ids]\n",
    "\n",
    "    if not selected_ids:\n",
    "        raise ValueError(\"No valid sequence_id selected.\")\n",
    "\n",
    "    print(f\"\\nSelected {len(selected_ids)} sequence(s): {selected_ids}\")\n",
    "\n",
    "    # Filter entropy data to selected IDs\n",
    "    df_entropy_filtered = df_entropy[df_entropy[\"sequence_id\"].isin(selected_ids)]\n",
    "\n",
    "    merge_cols = [\"sequence_id\", \"sequence\"]\n",
    "    if \"sequence\" in df_sequences.columns:\n",
    "        df_merged = df_entropy_filtered.merge(\n",
    "            df_sequences[merge_cols], on=\"sequence_id\", how=\"inner\"\n",
    "        )\n",
    "    else:\n",
    "        df_merged = df_entropy_filtered.copy()\n",
    "\n",
    "    if \"name\" in df_metadata.columns:\n",
    "        df_merged = df_merged.merge(\n",
    "            df_metadata[[\"sequence_id\", \"name\"]], on=\"sequence_id\", how=\"left\"\n",
    "        )\n",
    "\n",
    "    value_columns = [\n",
    "        c for c in df_entropy.columns if c != \"sequence_id\"\n",
    "    ]\n",
    "    print(f\"Format A detected (wide table). Value columns: {value_columns}\")\n",
    "    print(f\"Merged rows: {len(df_merged)}\")\n",
    "    display(df_merged.head())\n",
    "    ENTROPY_FORMAT = \"wide\"\n",
    "\n",
    "else:\n",
    "    # Format B -- per-residue file without sequence_id\n",
    "    # Try to extract sequence_id from the entropy filename\n",
    "    # Pipeline exports files named like: entropy_per_residue_{seq_id}.csv\n",
    "    import re\n",
    "    match = re.search(r\"entropy_per_residue_(.+)\\.csv\", entropy_filename)\n",
    "    if match:\n",
    "        inferred_seq_id = match.group(1)\n",
    "    else:\n",
    "        inferred_seq_id = Path(entropy_filename).stem\n",
    "\n",
    "    print(f\"Format B detected (per-residue). Inferred sequence_id: {inferred_seq_id}\")\n",
    "    override = input(f\"Use this sequence_id? (press Enter to accept, or type a new one): \").strip()\n",
    "    if override:\n",
    "        inferred_seq_id = override\n",
    "        print(f\"Using sequence_id: {inferred_seq_id}\")\n",
    "\n",
    "    # Determine which column holds the values\n",
    "    if \"entropy\" in df_entropy.columns:\n",
    "        value_col = \"entropy\"\n",
    "    else:\n",
    "        # Use the first numeric column that isn't residue_position\n",
    "        numeric_cols = [\n",
    "            c for c in df_entropy.select_dtypes(include=\"number\").columns\n",
    "            if c != \"residue_position\"\n",
    "        ]\n",
    "        value_col = numeric_cols[0] if numeric_cols else df_entropy.columns[-1]\n",
    "\n",
    "    print(f\"Using value column: '{value_col}'\")\n",
    "    print(f\"Residues: {len(df_entropy)}\")\n",
    "\n",
    "    # Look up the name from metadata\n",
    "    label = inferred_seq_id\n",
    "    if \"name\" in df_metadata.columns and \"sequence_id\" in df_metadata.columns:\n",
    "        name_match = df_metadata.loc[\n",
    "            df_metadata[\"sequence_id\"] == inferred_seq_id, \"name\"\n",
    "        ]\n",
    "        if len(name_match):\n",
    "            label = name_match.iloc[0]\n",
    "\n",
    "    # Store for the generation step\n",
    "    df_merged = None\n",
    "    per_residue_info = {\n",
    "        \"seq_id\": inferred_seq_id,\n",
    "        \"label\": label,\n",
    "        \"values\": df_entropy[value_col].values.astype(float),\n",
    "    }\n",
    "    value_columns = None\n",
    "    ENTROPY_FORMAT = \"per_residue\"\n",
    "    display(df_entropy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LXqXapPaeQo"
   },
   "source": [
    "## 3. Configure color mapping\n",
    "\n",
    "Adjust these parameters as needed before generating the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhRoMu5RaeQo",
    "outputId": "e2e621a0-3663-40a8-a9de-377949d719ee"
   },
   "outputs": [],
   "source": "# --- Interactive Configuration ---\n\n# Shared settings (apply to all files)\nif ENTROPY_FORMAT == \"wide\" and value_columns:\n    print(f\"Value columns: {value_columns}\")\n\nCMAP_NAME = input(\"Colormap name [Greys]: \").strip() or \"Greys\"\n\nprint(\"Transform methods: none, log, minmax, quantile, power, standard, robust\")\nTRANSFORM_METHOD = input(\"Transform method [none]: \").strip() or \"none\"\n\n_color = input(\"Enable color mapping? (y/n) [y]: \").strip().lower()\nCOLOR = _color not in (\"n\", \"no\")\n\nCOLOR_INVERT = False\nif COLOR:\n    _ci = input(\"Invert colormap? (y/n) [n]: \").strip().lower()\n    COLOR_INVERT = _ci in (\"y\", \"yes\")\n\n_trans = input(\"Enable transparency mapping? (y/n) [n]: \").strip().lower()\nTRANSPARENCY = _trans in (\"y\", \"yes\")\n\nTRANSPARENCY_INVERT = False\nif TRANSPARENCY:\n    _ti = input(\"Invert transparency? (y/n) [n]: \").strip().lower()\n    TRANSPARENCY_INVERT = _ti in (\"y\", \"yes\")\n\n# Model/chain configuration depends on mode\nif MULTI_FILE_MODE and all_entropy_data:\n    # Multi-file mode: model/chain already set per file, allow editing\n    print(\"\\n--- Per-file Model/Chain Configuration ---\")\n    print(\"Current assignments:\")\n    for i, data in enumerate(all_entropy_data):\n        spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n        print(f\"  {i+1}. {data['label']}: {spec}\")\n    \n    _edit = input(\"\\nEdit model/chain assignments? (y/n) [n]: \").strip().lower()\n    if _edit in (\"y\", \"yes\"):\n        for i, data in enumerate(all_entropy_data):\n            print(f\"\\n{data['label']} (currently #{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\") + \"):\")\n            _model = input(f\"  Model ID [{data['model']}]: \").strip()\n            if _model:\n                data['model'] = int(_model)\n            _chain = input(f\"  Chain ID [{data['chain'] or 'none'}]: \").strip()\n            if _chain.lower() == 'none':\n                data['chain'] = \"\"\n            elif _chain:\n                data['chain'] = _chain\n        \n        print(\"\\nUpdated assignments:\")\n        for i, data in enumerate(all_entropy_data):\n            spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n            print(f\"  {i+1}. {data['label']}: {spec}\")\n    \n    # These are not used in multi-file mode but define for consistency\n    MODEL = None\n    CHAIN = None\nelse:\n    # Single-file mode: one model/chain for all\n    _model = input(\"Model ID [1]: \").strip() or \"1\"\n    MODEL = int(_model)\n    CHAIN = input(\"Chain ID (e.g. A, B — leave blank for none): \").strip()\n\n# Summary\nprint(f\"\\n=== Configuration Summary ===\")\nprint(f\"Colormap: {CMAP_NAME}, Transform: {TRANSFORM_METHOD}\")\nprint(f\"Color: {COLOR} (invert={COLOR_INVERT}), Transparency: {TRANSPARENCY} (invert={TRANSPARENCY_INVERT})\")\n\nif MULTI_FILE_MODE and all_entropy_data:\n    print(\"Mode: Multi-file normalization\")\n    for data in all_entropy_data:\n        spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n        print(f\"  - {data['label']}: {spec}\")\nelse:\n    print(f\"Mode: Single-file\")\n    print(f\"Model: #{MODEL}\" + (f\"/{CHAIN}\" if CHAIN else \"\"))"
  },
  {
   "cell_type": "markdown",
   "source": "## 3b. Configure value cutoffs (optional)\n\nAfter transformation, you can filter which residues are included in the CXC output\nbased on their transformed values.\n\n**For quantile (rank-based) transforms:**\n- Values are ordinal ranks from 0 to 1\n- Specify rank ranges like `0-0.25` (bottom quartile) or `0.75-1.0` (top quartile)\n- Multiple ranges: `0-0.1, 0.9-1.0` (extreme 10% on both ends)\n\n**For other transforms (log, power, standard, robust, minmax, none):**\n- Values are continuous (rational numbers)\n- Statistics (mean, std, min, max) are displayed to help choose cutoffs\n- Specify value ranges based on the transformed values",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- Value Cutoff Configuration ---\n# This cell computes statistics on transformed values and allows filtering\n\n# Dictionary to store masks: key = seq_id or index, value = boolean mask array\nresidue_masks = {}\nUSE_CUTOFFS = False\n\n_use_cutoffs = input(\"Apply value cutoffs to filter residues? (y/n) [n]: \").strip().lower()\nUSE_CUTOFFS = _use_cutoffs in (\"y\", \"yes\")\n\nif USE_CUTOFFS:\n    is_rank_based = TRANSFORM_METHOD == \"quantile\"\n    \n    if is_rank_based:\n        print(\"\\n\" + \"=\"*60)\n        print(\"QUANTILE TRANSFORM (Rank-based)\")\n        print(\"=\"*60)\n        print(\"Values are ordinal ranks from 0 (lowest) to 1 (highest).\")\n        print(\"\\nExamples of rank range specifications:\")\n        print(\"  0-0.25        : Bottom quartile (lowest 25%)\")\n        print(\"  0.75-1.0      : Top quartile (highest 25%)\")\n        print(\"  0-0.1, 0.9-1.0: Extreme 10% on both ends\")\n        print(\"  0.25-0.75     : Middle 50% (interquartile range)\")\n        print(\"  all           : Include all residues (no filtering)\")\n    else:\n        print(\"\\n\" + \"=\"*60)\n        print(f\"TRANSFORM: {TRANSFORM_METHOD.upper() if TRANSFORM_METHOD != 'none' else 'NONE (raw values)'}\")\n        print(\"=\"*60)\n        print(\"Values are continuous. Statistics are shown to help choose cutoffs.\")\n        print(\"\\nExamples of value range specifications:\")\n        print(\"  -1.5-0.5      : Values between -1.5 and 0.5\")\n        print(\"  0-2, 5-10     : Multiple ranges (0 to 2 OR 5 to 10)\")\n        print(\"  all           : Include all residues (no filtering)\")\n    \n    # --- Multi-file mode ---\n    if MULTI_FILE_MODE and all_entropy_data:\n        # Fit transform scaler on combined data for consistent statistics\n        combined_values = np.concatenate([d[\"values\"] for d in all_entropy_data])\n        transform_scaler_for_stats = fit_scaler(combined_values, method=TRANSFORM_METHOD)\n        \n        print(\"\\n\" + \"-\"*60)\n        print(\"COMBINED STATISTICS (all files)\")\n        print(\"-\"*60)\n        combined_stats = compute_value_statistics(\n            combined_values,\n            transform_method=TRANSFORM_METHOD,\n            transform_scaler=transform_scaler_for_stats,\n        )\n        combined_stats[\"transform_method\"] = TRANSFORM_METHOD\n        print(display_statistics_summary(combined_stats, label=\"All Files Combined\"))\n        \n        # Show per-file statistics\n        print(\"\\n\" + \"-\"*60)\n        print(\"PER-FILE STATISTICS\")\n        print(\"-\"*60)\n        for i, data in enumerate(all_entropy_data):\n            stats = compute_value_statistics(\n                data[\"values\"],\n                transform_method=TRANSFORM_METHOD,\n                transform_scaler=transform_scaler_for_stats,\n            )\n            stats[\"transform_method\"] = TRANSFORM_METHOD\n            print(f\"\\n{i+1}. {data['label']} ({data['seq_id']}):\")\n            print(f\"   Residues: {stats['n_residues']}\")\n            print(f\"   Range: {stats['min']:.4f} - {stats['max']:.4f}\")\n            print(f\"   Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n            # Store transformed values for filtering\n            data[\"transformed_values\"] = stats[\"transformed_values\"]\n        \n        # Ask for range specification\n        print(\"\\n\" + \"-\"*60)\n        print(\"SPECIFY CUTOFF RANGES\")\n        print(\"-\"*60)\n        \n        _same_range = input(\"Use same range for all files? (y/n) [y]: \").strip().lower()\n        use_same_range = _same_range not in (\"n\", \"no\")\n        \n        if use_same_range:\n            if is_rank_based:\n                range_str = input(\"Rank range(s) for all files [all]: \").strip() or \"all\"\n            else:\n                range_str = input(\"Value range(s) for all files [all]: \").strip() or \"all\"\n            \n            try:\n                ranges = parse_range_string(range_str)\n                if ranges:\n                    print(f\"\\nApplying ranges: {ranges}\")\n                    for data in all_entropy_data:\n                        mask = create_value_mask(data[\"transformed_values\"], ranges)\n                        residue_masks[data[\"seq_id\"]] = mask\n                        included = np.sum(mask)\n                        print(f\"  {data['label']}: {included}/{len(mask)} residues included ({100*included/len(mask):.1f}%)\")\n                else:\n                    print(\"\\nNo filtering - all residues will be included.\")\n            except ValueError as e:\n                print(f\"Error parsing range: {e}\")\n                print(\"No filtering will be applied.\")\n        else:\n            # Per-file range specification\n            for data in all_entropy_data:\n                print(f\"\\n{data['label']} ({data['seq_id']}):\")\n                if is_rank_based:\n                    range_str = input(f\"  Rank range(s) [all]: \").strip() or \"all\"\n                else:\n                    range_str = input(f\"  Value range(s) [all]: \").strip() or \"all\"\n                \n                try:\n                    ranges = parse_range_string(range_str)\n                    if ranges:\n                        mask = create_value_mask(data[\"transformed_values\"], ranges)\n                        residue_masks[data[\"seq_id\"]] = mask\n                        included = np.sum(mask)\n                        print(f\"    -> {included}/{len(mask)} residues included ({100*included/len(mask):.1f}%)\")\n                    else:\n                        print(f\"    -> All residues included (no filtering)\")\n                except ValueError as e:\n                    print(f\"    Error: {e}. No filtering for this file.\")\n    \n    # --- Single-file mode: per-residue format ---\n    elif ENTROPY_FORMAT == \"per_residue\":\n        values = per_residue_info[\"values\"]\n        stats = compute_value_statistics(values, transform_method=TRANSFORM_METHOD)\n        stats[\"transform_method\"] = TRANSFORM_METHOD\n        print(\"\\n\" + display_statistics_summary(stats, label=per_residue_info[\"label\"]))\n        \n        # Store transformed values\n        per_residue_info[\"transformed_values\"] = stats[\"transformed_values\"]\n        \n        print(\"\\n\" + \"-\"*60)\n        if is_rank_based:\n            range_str = input(\"Rank range(s) to include [all]: \").strip() or \"all\"\n        else:\n            range_str = input(\"Value range(s) to include [all]: \").strip() or \"all\"\n        \n        try:\n            ranges = parse_range_string(range_str)\n            if ranges:\n                mask = create_value_mask(stats[\"transformed_values\"], ranges)\n                residue_masks[per_residue_info[\"seq_id\"]] = mask\n                included = np.sum(mask)\n                print(f\"-> {included}/{len(mask)} residues included ({100*included/len(mask):.1f}%)\")\n            else:\n                print(\"-> All residues included (no filtering)\")\n        except ValueError as e:\n            print(f\"Error parsing range: {e}\")\n            print(\"No filtering will be applied.\")\n    \n    # --- Single-file mode: wide format ---\n    elif ENTROPY_FORMAT == \"wide\":\n        print(\"\\nComputing statistics for each sequence...\")\n        \n        for _, row in df_merged.iterrows():\n            seq_id = row[\"sequence_id\"]\n            label = row.get(\"name\", seq_id) or seq_id\n            values = row[value_columns].values.astype(float)\n            \n            stats = compute_value_statistics(values, transform_method=TRANSFORM_METHOD)\n            stats[\"transform_method\"] = TRANSFORM_METHOD\n            print(f\"\\n{label} ({seq_id}):\")\n            print(f\"  Residues: {stats['n_residues']}\")\n            print(f\"  Range: {stats['min']:.4f} - {stats['max']:.4f}\")\n            print(f\"  Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n        \n        print(\"\\n\" + \"-\"*60)\n        _same_range = input(\"Use same range for all sequences? (y/n) [y]: \").strip().lower()\n        use_same_range = _same_range not in (\"n\", \"no\")\n        \n        if use_same_range:\n            if is_rank_based:\n                range_str = input(\"Rank range(s) for all sequences [all]: \").strip() or \"all\"\n            else:\n                range_str = input(\"Value range(s) for all sequences [all]: \").strip() or \"all\"\n            \n            try:\n                ranges = parse_range_string(range_str)\n                if ranges:\n                    print(f\"\\nApplying ranges: {ranges}\")\n                    for _, row in df_merged.iterrows():\n                        seq_id = row[\"sequence_id\"]\n                        values = row[value_columns].values.astype(float)\n                        transformed = scale_values(values, method=TRANSFORM_METHOD)\n                        mask = create_value_mask(transformed, ranges)\n                        residue_masks[seq_id] = mask\n                        included = np.sum(mask)\n                        print(f\"  {seq_id}: {included}/{len(mask)} residues ({100*included/len(mask):.1f}%)\")\n                else:\n                    print(\"\\nNo filtering - all residues will be included.\")\n            except ValueError as e:\n                print(f\"Error parsing range: {e}\")\n        else:\n            for _, row in df_merged.iterrows():\n                seq_id = row[\"sequence_id\"]\n                label = row.get(\"name\", seq_id) or seq_id\n                values = row[value_columns].values.astype(float)\n                \n                print(f\"\\n{label} ({seq_id}):\")\n                if is_rank_based:\n                    range_str = input(f\"  Rank range(s) [all]: \").strip() or \"all\"\n                else:\n                    range_str = input(f\"  Value range(s) [all]: \").strip() or \"all\"\n                \n                try:\n                    ranges = parse_range_string(range_str)\n                    if ranges:\n                        transformed = scale_values(values, method=TRANSFORM_METHOD)\n                        mask = create_value_mask(transformed, ranges)\n                        residue_masks[seq_id] = mask\n                        included = np.sum(mask)\n                        print(f\"    -> {included}/{len(mask)} residues ({100*included/len(mask):.1f}%)\")\n                    else:\n                        print(f\"    -> All residues included\")\n                except ValueError as e:\n                    print(f\"    Error: {e}\")\n    \n    # Summary\n    if residue_masks:\n        print(\"\\n\" + \"=\"*60)\n        print(\"CUTOFF SUMMARY\")\n        print(\"=\"*60)\n        total_residues = sum(len(m) for m in residue_masks.values())\n        total_included = sum(np.sum(m) for m in residue_masks.values())\n        print(f\"Total: {total_included}/{total_residues} residues will be included ({100*total_included/total_residues:.1f}%)\")\n    else:\n        print(\"\\nNo cutoffs applied - all residues will be included.\")\nelse:\n    print(\"Cutoffs disabled - all residues will be included in CXC output.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYrwjdYFaeQo"
   },
   "source": [
    "## 4. Generate `.cxc` scripts (one per sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVaYSPPjaeQo",
    "outputId": "3f820b88-b86d-447c-c299-ca0d0e5ce0e9"
   },
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\noutput_dir = \"cxc_output\"\nos.makedirs(output_dir, exist_ok=True)\n\ngenerated_files = []\n\ndef _make_cxc(values, label, seq_id, model, chain, mask=None):\n    \"\"\"Generate a .cxc file for one sequence (single-file mode).\"\"\"\n    script = generate_chimerax_script(\n        values,\n        cmap_name=CMAP_NAME,\n        transform_method=TRANSFORM_METHOD,\n        color=COLOR,\n        color_invert=COLOR_INVERT,\n        transparency=TRANSPARENCY,\n        transparency_invert=TRANSPARENCY_INVERT,\n        model=model,\n        chain=chain,\n        residue_mask=mask,\n    )\n    safe_label = str(label).replace(\" \", \"_\").replace(\"/\", \"_\")\n    chain_str = chain if chain else \"\"\n    cmap_str = f\"{CMAP_NAME}_i\" if COLOR_INVERT else CMAP_NAME\n    # Add \"filtered\" suffix if mask is applied\n    filter_str = \"_filtered\" if mask is not None and not np.all(mask) else \"\"\n    out_path = os.path.join(output_dir, f\"{safe_label}_{seq_id}_{model}{chain_str}_{cmap_str}_{TRANSFORM_METHOD}{filter_str}.cxc\")\n    write_chimerax_script(script, out_path)\n    return out_path\n\ndef _make_cxc_with_scalers(values, label, seq_id, model, chain, transform_scaler, minmax_scaler, mask=None):\n    \"\"\"Generate a .cxc file using pre-fitted scalers (multi-file mode).\"\"\"\n    script = generate_chimerax_script_with_scalers(\n        values,\n        transform_scaler=transform_scaler,\n        minmax_scaler=minmax_scaler,\n        cmap_name=CMAP_NAME,\n        color=COLOR,\n        color_invert=COLOR_INVERT,\n        transparency=TRANSPARENCY,\n        transparency_invert=TRANSPARENCY_INVERT,\n        model=model,\n        chain=chain,\n        residue_mask=mask,\n    )\n    safe_label = str(label).replace(\" \", \"_\").replace(\"/\", \"_\")\n    chain_str = chain if chain else \"\"\n    cmap_str = f\"{CMAP_NAME}_i\" if COLOR_INVERT else CMAP_NAME\n    # Add \"filtered\" suffix if mask is applied\n    filter_str = \"_filtered\" if mask is not None and not np.all(mask) else \"\"\n    out_path = os.path.join(output_dir, f\"{safe_label}_{seq_id}_{model}{chain_str}_{cmap_str}_{TRANSFORM_METHOD}_multifile{filter_str}.cxc\")\n    write_chimerax_script(script, out_path)\n    return out_path\n\n# --- Multi-file mode: fit scalers on combined data ---\nif MULTI_FILE_MODE and all_entropy_data:\n    print(\"=== Multi-file normalization ===\")\n    \n    # Combine all values\n    combined_values = np.concatenate([d[\"values\"] for d in all_entropy_data])\n    print(f\"Combined {len(all_entropy_data)} files: {len(combined_values)} total residues\")\n    print(f\"  Min: {combined_values.min():.4f}, Max: {combined_values.max():.4f}\")\n    print(f\"  Mean: {combined_values.mean():.4f}, Std: {combined_values.std():.4f}\")\n    \n    # Fit transform scaler on combined data\n    transform_scaler = fit_scaler(combined_values, method=TRANSFORM_METHOD)\n    \n    # Apply transform to combined data, then fit minmax scaler\n    if transform_scaler is not None:\n        scaled_combined = scale_values_with_scaler(combined_values, transform_scaler)\n    else:\n        scaled_combined = combined_values\n    \n    minmax_scaler = fit_minmax_scaler(scaled_combined)\n    print(f\"Fitted scalers on combined data (transform={TRANSFORM_METHOD})\")\n    \n    if residue_masks:\n        print(f\"Applying cutoff filters to {len(residue_masks)} file(s)\\n\")\n    else:\n        print(\"No cutoff filters applied\\n\")\n    \n    # Generate .cxc for each sequence using the shared scalers and per-file model/chain\n    for data in all_entropy_data:\n        # Get mask for this sequence if available\n        mask = residue_masks.get(data[\"seq_id\"], None)\n        \n        out_path = _make_cxc_with_scalers(\n            data[\"values\"],\n            data[\"label\"],\n            data[\"seq_id\"],\n            data[\"model\"],\n            data[\"chain\"],\n            transform_scaler,\n            minmax_scaler,\n            mask=mask,\n        )\n        generated_files.append(out_path)\n        spec = f\"#{data['model']}\" + (f\"/{data['chain']}\" if data['chain'] else \"\")\n        if mask is not None and not np.all(mask):\n            included = np.sum(mask)\n            print(f\"  Created: {out_path} ({spec}) [{included}/{len(mask)} residues]\")\n        else:\n            print(f\"  Created: {out_path} ({spec})\")\n\n# --- Single-file mode ---\nelif ENTROPY_FORMAT == \"wide\":\n    # One row per sequence -- iterate over df_merged\n    for _, row in df_merged.iterrows():\n        seq_id = row[\"sequence_id\"]\n        label = row.get(\"name\", seq_id) or seq_id\n        values = row[value_columns].values.astype(float)\n        \n        # Get mask for this sequence if available\n        mask = residue_masks.get(seq_id, None)\n\n        out_path = _make_cxc(values, label, seq_id, MODEL, CHAIN, mask=mask)\n        generated_files.append(out_path)\n        if mask is not None and not np.all(mask):\n            included = np.sum(mask)\n            print(f\"  Created: {out_path} [{included}/{len(mask)} residues]\")\n        else:\n            print(f\"  Created: {out_path}\")\nelse:\n    # Per-residue format -- single sequence\n    # Get mask for this sequence if available\n    mask = residue_masks.get(per_residue_info[\"seq_id\"], None)\n    \n    out_path = _make_cxc(\n        per_residue_info[\"values\"],\n        per_residue_info[\"label\"],\n        per_residue_info[\"seq_id\"],\n        MODEL,\n        CHAIN,\n        mask=mask,\n    )\n    generated_files.append(out_path)\n    if mask is not None and not np.all(mask):\n        included = np.sum(mask)\n        print(f\"  Created: {out_path} [{included}/{len(mask)} residues]\")\n    else:\n        print(f\"  Created: {out_path}\")\n\nprint(f\"\\nGenerated {len(generated_files)} .cxc file(s).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15_ECcEKaeQo"
   },
   "source": [
    "## 5. Download `.cxc` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "PKEKsbxJaeQo",
    "outputId": "ea8c67f4-121f-4443-cb47-e708212a45f1"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_22d60619-4f6a-4a14-b5d9-133489664103\", \"PB1_0ea58344488f_1a_Blues.cxc\", 40574)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in generated_files:\n",
    "    files.download(f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
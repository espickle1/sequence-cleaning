{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß¨ FASTA File Cleaner\n",
    "\n",
    "A simple tool to clean and consolidate FASTA files with amino acid sequences.\n",
    "\n",
    "## How to use:\n",
    "1. **Run all cells** (Runtime ‚Üí Run all)\n",
    "2. **Upload your FASTA file(s)** using the button below\n",
    "3. **Click \"Process Files\"** to clean and parse your sequences\n",
    "4. **Download** the resulting CSV files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Run this cell first!\n",
    "# ============================================================\n",
    "\n",
    "import hashlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files as colab_files\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úÖ Running in local Jupyter environment\")\n",
    "\n",
    "# Import widgets for file upload\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    print(f\"‚úÖ Widgets loaded (ipywidgets version: {widgets.__version__})\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ipywidgets not found. Installing...\")\n",
    "    !pip install ipywidgets\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "\n",
    "print(\"\\nüéâ Setup complete! Proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "# Canonical 20 amino acids\n",
    "CANONICAL_AA = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "\n",
    "def clean_sequence(sequence: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean an amino acid sequence.\n",
    "    - Convert to uppercase\n",
    "    - Replace non-canonical amino acids with underscore\n",
    "    - Remove whitespace and newlines\n",
    "    \"\"\"\n",
    "    sequence = sequence.upper().replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "    cleaned = \"\"\n",
    "    for char in sequence:\n",
    "        if char in CANONICAL_AA:\n",
    "            cleaned += char\n",
    "        elif char.isalpha():  # Non-canonical amino acid letter\n",
    "            cleaned += \"_\"\n",
    "        # Skip non-letter characters (numbers, symbols, etc.)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def hash_sequence(sequence: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique ID for a sequence using SHA-256.\n",
    "    Returns first 12 characters for readability.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(sequence.encode()).hexdigest()[:12]\n",
    "\n",
    "\n",
    "def parse_header(header: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse FASTA header to extract metadata fields.\n",
    "    Handles common formats:\n",
    "    - UniProt: sp|P12345|PROTEIN_NAME|date|...\n",
    "    - GenBank: gb|ABC123|NAME|...\n",
    "    - Custom: delimited by | ; / or tab\n",
    "    Attempts to identify 'name' and 'date' fields.\n",
    "    \"\"\"\n",
    "    # Remove leading > if present\n",
    "    header = header.lstrip(\">\")\n",
    "    \n",
    "    result = {\n",
    "        \"original_header\": header,\n",
    "        \"name\": \"\",\n",
    "        \"date\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Date pattern: YYYY-MM-DD, DD/MM/YYYY, MM-DD-YYYY, etc.\n",
    "    date_pattern = re.compile(\n",
    "        r\"\\b(\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\\b\"\n",
    "    )\n",
    "    \n",
    "    # Known database prefixes to skip when finding name\n",
    "    db_prefixes = {\"sp\", \"tr\", \"gb\", \"ref\", \"emb\", \"dbj\", \"pir\", \"prf\", \"uniprot\"}\n",
    "    \n",
    "    # Try splitting by common delimiters\n",
    "    delimiters = [\"|\", \";\", \"/\", \"\\t\"]\n",
    "    fields = [header]\n",
    "    \n",
    "    for delim in delimiters:\n",
    "        if delim in header:\n",
    "            fields = [f.strip() for f in header.split(delim)]\n",
    "            break\n",
    "    \n",
    "    # Process fields to find name and date\n",
    "    extra_fields = []\n",
    "    name_found = False\n",
    "    \n",
    "    for field in fields:\n",
    "        field = field.strip()\n",
    "        if not field:\n",
    "            continue\n",
    "        \n",
    "        # Skip database prefixes\n",
    "        if field.lower() in db_prefixes:\n",
    "            continue\n",
    "        \n",
    "        # Skip accession numbers (mostly alphanumeric, short)\n",
    "        if re.match(r\"^[A-Z0-9]{4,12}$\", field) and not name_found:\n",
    "            extra_fields.append(field)  # Keep as extra field\n",
    "            continue\n",
    "        \n",
    "        # Check for date\n",
    "        date_match = date_pattern.search(field)\n",
    "        if date_match and not result[\"date\"]:\n",
    "            result[\"date\"] = date_match.group(1)\n",
    "            # If field is just the date, don't add to extras\n",
    "            if field == date_match.group(1):\n",
    "                continue\n",
    "        \n",
    "        # First meaningful field is the name\n",
    "        if not name_found:\n",
    "            result[\"name\"] = field\n",
    "            name_found = True\n",
    "        else:\n",
    "            extra_fields.append(field)\n",
    "    \n",
    "    # Add extra fields with numbered keys\n",
    "    for i, field in enumerate(extra_fields, 1):\n",
    "        result[f\"field_{i}\"] = field\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def parse_fasta(content: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse FASTA format content.\n",
    "    Returns list of (header, sequence) tuples.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    current_header = None\n",
    "    current_seq = []\n",
    "    \n",
    "    for line in content.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        if line.startswith(\">\"):\n",
    "            # Save previous sequence if exists\n",
    "            if current_header is not None:\n",
    "                sequences.append((current_header, \"\".join(current_seq)))\n",
    "            current_header = line[1:]  # Remove >\n",
    "            current_seq = []\n",
    "        else:\n",
    "            current_seq.append(line)\n",
    "    \n",
    "    # Don't forget the last sequence\n",
    "    if current_header is not None:\n",
    "        sequences.append((current_header, \"\".join(current_seq)))\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "\n",
    "def handle_duplicate_metadata(metadata_list: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Handle entries with identical metadata but distinct sequences.\n",
    "    Appends version marker to name field (e.g., _v2, _v3).\n",
    "    \"\"\"\n",
    "    # Group by (name, date) to find duplicates\n",
    "    seen = defaultdict(list)\n",
    "    \n",
    "    for i, meta in enumerate(metadata_list):\n",
    "        key = (meta.get(\"name\", \"\"), meta.get(\"date\", \"\"))\n",
    "        seen[key].append(i)\n",
    "    \n",
    "    # Mark duplicates\n",
    "    for key, indices in seen.items():\n",
    "        if len(indices) > 1:\n",
    "            # Check if sequences are actually different\n",
    "            seq_ids = [metadata_list[i][\"sequence_id\"] for i in indices]\n",
    "            if len(set(seq_ids)) > 1:  # Different sequences\n",
    "                for version, idx in enumerate(indices, 1):\n",
    "                    if version > 1:  # Don't mark the first one\n",
    "                        original_name = metadata_list[idx].get(\"name\", \"\")\n",
    "                        metadata_list[idx][\"name\"] = f\"{original_name}_v{version}\"\n",
    "    \n",
    "    return metadata_list\n",
    "\n",
    "\n",
    "def get_file_content(data) -> str:\n",
    "    \"\"\"\n",
    "    Extract file content as string, handling different ipywidgets versions.\n",
    "    \"\"\"\n",
    "    if isinstance(data, bytes):\n",
    "        return data.decode(\"utf-8\")\n",
    "    elif hasattr(data, \"tobytes\"):\n",
    "        return data.tobytes().decode(\"utf-8\")\n",
    "    elif isinstance(data, str):\n",
    "        return data\n",
    "    else:\n",
    "        return str(data)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FILE UPLOAD WIDGET\n",
    "# ============================================================\n",
    "\n",
    "# Storage for uploaded files\n",
    "uploaded_files = {}\n",
    "\n",
    "# Create upload widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept=\".fasta,.fa,.faa,.txt\",  # Accept common FASTA extensions\n",
    "    multiple=True,\n",
    "    description=\"Upload FASTA\",\n",
    "    button_style=\"primary\",\n",
    "    layout=widgets.Layout(width=\"200px\")\n",
    ")\n",
    "\n",
    "# Output area for status messages\n",
    "upload_output = widgets.Output()\n",
    "\n",
    "def on_upload_change(change):\n",
    "    \"\"\"Handle file upload events - compatible with ipywidgets 7.x and 8.x.\"\"\"\n",
    "    global uploaded_files\n",
    "    with upload_output:\n",
    "        clear_output()\n",
    "        new_value = change[\"new\"]\n",
    "        \n",
    "        if not new_value:\n",
    "            return\n",
    "        \n",
    "        # Handle ipywidgets 8.x format (dict with filename keys)\n",
    "        if isinstance(new_value, dict):\n",
    "            for filename, file_data in new_value.items():\n",
    "                # file_data can be dict with 'content' key or direct bytes\n",
    "                if isinstance(file_data, dict) and \"content\" in file_data:\n",
    "                    content = get_file_content(file_data[\"content\"])\n",
    "                else:\n",
    "                    content = get_file_content(file_data)\n",
    "                uploaded_files[filename] = content\n",
    "                print(f\"üìÑ Uploaded: {filename} ({len(content):,} bytes)\")\n",
    "        \n",
    "        # Handle ipywidgets 7.x format (tuple/list of dicts)\n",
    "        elif isinstance(new_value, (list, tuple)):\n",
    "            for file_info in new_value:\n",
    "                if isinstance(file_info, dict):\n",
    "                    name = file_info.get(\"name\", \"unknown\")\n",
    "                    content = get_file_content(file_info.get(\"content\", b\"\"))\n",
    "                    uploaded_files[name] = content\n",
    "                    print(f\"üìÑ Uploaded: {name} ({len(content):,} bytes)\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Total files ready: {len(uploaded_files)}\")\n",
    "\n",
    "upload_widget.observe(on_upload_change, names=\"value\")\n",
    "\n",
    "# Display\n",
    "display(HTML(\"<h3>üì§ Step 1: Upload Your FASTA Files</h3>\"))\n",
    "display(HTML(\"<p>Click the button below to select one or more FASTA files:</p>\"))\n",
    "display(upload_widget)\n",
    "display(upload_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROCESS FILES\n",
    "# ============================================================\n",
    "\n",
    "# Output area for processing\n",
    "process_output = widgets.Output()\n",
    "\n",
    "# Storage for results\n",
    "sequences_df = None\n",
    "metadata_df = None\n",
    "\n",
    "\n",
    "def process_files(btn):\n",
    "    \"\"\"Process all uploaded FASTA files.\"\"\"\n",
    "    global sequences_df, metadata_df\n",
    "    \n",
    "    with process_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if not uploaded_files:\n",
    "            print(\"‚ö†Ô∏è No files uploaded! Please upload FASTA files first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"üîÑ Processing files...\\n\")\n",
    "        \n",
    "        all_sequences = []  # (sequence_id, cleaned_seq, length)\n",
    "        all_metadata = []   # {sequence_id, original_header, name, date, ...}\n",
    "        seen_sequences = {} # cleaned_seq -> sequence_id (for deduplication)\n",
    "        \n",
    "        for filename, content in uploaded_files.items():\n",
    "            print(f\"üìÑ Processing: {filename}\")\n",
    "            parsed = parse_fasta(content)\n",
    "            print(f\"   Found {len(parsed)} sequences\")\n",
    "            \n",
    "            for header, raw_seq in parsed:\n",
    "                # Clean sequence\n",
    "                cleaned = clean_sequence(raw_seq)\n",
    "                \n",
    "                if not cleaned:\n",
    "                    print(f\"   ‚ö†Ô∏è Skipping empty sequence: {header[:50]}...\")\n",
    "                    continue\n",
    "                \n",
    "                # Get or create sequence ID\n",
    "                if cleaned in seen_sequences:\n",
    "                    seq_id = seen_sequences[cleaned]\n",
    "                else:\n",
    "                    seq_id = hash_sequence(cleaned)\n",
    "                    seen_sequences[cleaned] = seq_id\n",
    "                    all_sequences.append({\n",
    "                        \"sequence_id\": seq_id,\n",
    "                        \"sequence\": cleaned,\n",
    "                        \"length\": len(cleaned)\n",
    "                    })\n",
    "                \n",
    "                # Parse metadata\n",
    "                meta = parse_header(header)\n",
    "                meta[\"sequence_id\"] = seq_id\n",
    "                meta[\"source_file\"] = filename\n",
    "                all_metadata.append(meta)\n",
    "        \n",
    "        # Handle duplicate metadata with different sequences\n",
    "        all_metadata = handle_duplicate_metadata(all_metadata)\n",
    "        \n",
    "        # Create DataFrames\n",
    "        sequences_df = pd.DataFrame(all_sequences)\n",
    "        metadata_df = pd.DataFrame(all_metadata)\n",
    "        \n",
    "        # Reorder metadata columns\n",
    "        priority_cols = [\"sequence_id\", \"original_header\", \"name\", \"date\", \"source_file\"]\n",
    "        other_cols = [c for c in metadata_df.columns if c not in priority_cols]\n",
    "        metadata_df = metadata_df[priority_cols + other_cols]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"‚úÖ PROCESSING COMPLETE!\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\nüìä Results:\")\n",
    "        print(f\"   ‚Ä¢ Unique sequences: {len(sequences_df)}\")\n",
    "        print(f\"   ‚Ä¢ Metadata entries: {len(metadata_df)}\")\n",
    "        \n",
    "        # Show preview\n",
    "        print(\"\\nüìã Sequences Preview:\")\n",
    "        display(sequences_df.head())\n",
    "        \n",
    "        print(\"\\nüìã Metadata Preview:\")\n",
    "        display(metadata_df.head())\n",
    "        \n",
    "        print(\"\\nüëá Proceed to the next cell to download the CSV files.\")\n",
    "\n",
    "\n",
    "# Create process button\n",
    "process_btn = widgets.Button(\n",
    "    description=\"üî¨ Process Files\",\n",
    "    button_style=\"success\",\n",
    "    layout=widgets.Layout(width=\"200px\", height=\"40px\")\n",
    ")\n",
    "process_btn.on_click(process_files)\n",
    "\n",
    "# Display\n",
    "display(HTML(\"<h3>‚öôÔ∏è Step 2: Process Your Files</h3>\"))\n",
    "display(HTML(\"<p>Click the button below to clean and parse your sequences:</p>\"))\n",
    "display(process_btn)\n",
    "display(process_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD RESULTS\n",
    "# ============================================================\n",
    "\n",
    "download_output = widgets.Output()\n",
    "\n",
    "\n",
    "def download_sequences(btn):\n",
    "    \"\"\"Download sequences CSV.\"\"\"\n",
    "    with download_output:\n",
    "        clear_output()\n",
    "        if sequences_df is None or sequences_df.empty:\n",
    "            print(\"‚ö†Ô∏è No sequences to download. Process files first!\")\n",
    "            return\n",
    "        \n",
    "        filename = \"sequences.csv\"\n",
    "        if IN_COLAB:\n",
    "            sequences_df.to_csv(filename, index=False)\n",
    "            colab_files.download(filename)\n",
    "            print(f\"‚úÖ Downloading {filename}...\")\n",
    "        else:\n",
    "            sequences_df.to_csv(filename, index=False)\n",
    "            print(f\"‚úÖ Saved to: {filename}\")\n",
    "\n",
    "\n",
    "def download_metadata(btn):\n",
    "    \"\"\"Download metadata CSV.\"\"\"\n",
    "    with download_output:\n",
    "        clear_output()\n",
    "        if metadata_df is None or metadata_df.empty:\n",
    "            print(\"‚ö†Ô∏è No metadata to download. Process files first!\")\n",
    "            return\n",
    "        \n",
    "        filename = \"metadata.csv\"\n",
    "        if IN_COLAB:\n",
    "            metadata_df.to_csv(filename, index=False)\n",
    "            colab_files.download(filename)\n",
    "            print(f\"‚úÖ Downloading {filename}...\")\n",
    "        else:\n",
    "            metadata_df.to_csv(filename, index=False)\n",
    "            print(f\"‚úÖ Saved to: {filename}\")\n",
    "\n",
    "\n",
    "def download_both(btn):\n",
    "    \"\"\"Download both CSV files.\"\"\"\n",
    "    download_sequences(btn)\n",
    "    download_metadata(btn)\n",
    "\n",
    "\n",
    "# Create download buttons\n",
    "seq_btn = widgets.Button(\n",
    "    description=\"üì• Sequences CSV\",\n",
    "    button_style=\"info\",\n",
    "    layout=widgets.Layout(width=\"150px\")\n",
    ")\n",
    "seq_btn.on_click(download_sequences)\n",
    "\n",
    "meta_btn = widgets.Button(\n",
    "    description=\"üì• Metadata CSV\",\n",
    "    button_style=\"info\",\n",
    "    layout=widgets.Layout(width=\"150px\")\n",
    ")\n",
    "meta_btn.on_click(download_metadata)\n",
    "\n",
    "both_btn = widgets.Button(\n",
    "    description=\"üì• Download Both\",\n",
    "    button_style=\"warning\",\n",
    "    layout=widgets.Layout(width=\"150px\")\n",
    ")\n",
    "both_btn.on_click(download_both)\n",
    "\n",
    "# Display\n",
    "display(HTML(\"<h3>üíæ Step 3: Download Your Results</h3>\"))\n",
    "display(HTML(\"<p>Click the buttons below to download your cleaned data:</p>\"))\n",
    "display(widgets.HBox([seq_btn, meta_btn, both_btn]))\n",
    "display(download_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Output File Descriptions\n",
    "\n",
    "### `sequences.csv`\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `sequence_id` | Unique 12-character hash ID for each sequence |\n",
    "| `sequence` | Cleaned amino acid sequence (non-canonical AAs replaced with `_`) |\n",
    "| `length` | Number of amino acids in the sequence |\n",
    "\n",
    "### `metadata.csv`\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `sequence_id` | Links to `sequences.csv` |\n",
    "| `original_header` | Original FASTA header line |\n",
    "| `name` | Extracted protein/sequence name |\n",
    "| `date` | Extracted date (if present) |\n",
    "| `source_file` | Original filename |\n",
    "| `field_N` | Additional parsed fields from header |\n",
    "\n",
    "> **Note:** Empty cells indicate missing data in the original header. These are intentionally preserved as empty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}